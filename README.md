# IDEAlign
Benchmarking method for comparing LLMs to humans in open-ended, interpretive annotations
